<h3>Algorithms and Data Structures</h3>  
<p>
    <ol style="list-style-type:lower-alpha">
      <li>Study <a href="http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming"><i>"The Art of Computing"</i> by Donald Knuth</a>.  Never read it, always wanted to.  OR...</li>
      <li><a href="http://www.youtube.com/playlist?list=PLE621E25B3BF8B9D1&feature=plcp">Youtube lectures</a> by <a href="https://wiki.cse.unsw.edu.au/gandt/RichardBuckland/CV">Richard Buckland</a></li>
    </ol>
<br />
<br />
<h2>UNSW Comp 1927 Data Structures and Algorithms - presented by Prof. Richard Buckland</h2>
<ol>
  <li>Lecture 8 - 10</li>
    <ol style="list-style-type: lower-alpha;">
      <li>Merge Sort: Calculating complexity</li>
        <ol style="list-style-type: lower-roman;">
          <li>Let T(n) be related to the time-based sorting solution function with respect to the number of elements "n"</li>
          <li>Since merge sort involves splitting the set of n into two equal sets, sorting them and then merging them (recursively), then</li>
          <li><b>T(n) = n + T(n/2) + T(n/2)</b></li>
          <li>so that for any call to sort set "n", T(n) will take 1 pass to find the mid-point + the same T function but acting on each half as the function is called again for each of those halves</li>
          <li>given this, we can show that this equation is true by substituting nlogn for T(n), therefore</li>
          <li>nlogn = n + (n/2)log(n/2) + (n/2)log(n/2) ... reducing this gives</li>
          <li>nlogn = n + nlog(n/2) ...further reduction gives </li>
          <li>nlogn = n + n(logn - log2) ...reducing again to</li>
          <li>nlogn = n + nlogn - n  ...which balances nicely as</li>
          <li>nlogn = nlogn</li>
          <li><b>Complexity of Merge Sort is O(nlogn)</b></li>
          <li> * note that the log base is 2</li>
        </ol>
      <li>How good is nlogn complexity></li>
        <ol style="list-style-type: lower-roman;">
          <li>if n = 1 billion then n^2 complexity gives 1 billion x 1 billion = 1 quintillion (1 x 10^18) </li>
          <li>if n = 1 billion then nlogn complexity gives 1 billion x log(1 billion) = 30 billion<li>
          <li><b>That is a big difference!!</b></li>
        </ol>
      <li>Disadvantages to Merge Sort:</li>
        <ol style="list-style-type: lower-roman;">
          <li>Recursion is resource heavy - big disadvantage for very large number sets</li>
          <li>nlogn can be improved upon in many cases</li>
        </ol>
      <li>Why base 2?  That is, why are computers base 2?</li>
        <ol style="list-style-type: lower-roman;">
          <li>Buckland said he wanted students to think on this and would answer it in a future lesson</li>
          <li>He did mention that base 2 predated the computer and mentioned Claude Shannon</li>
          <li>Claude Shannon was the father of Information Theory and the proposer, definer and solver of digitization - in base 2</li>
          <li>Without knowing it in too much detail, I imagine it was the Shannon's use of Boolean Logic to enable digitization that led to base 2</li>
        </ol>
      <li>Quick Sort</li>
        <ol style="list-style-type: lower-roman;">
          <li>Mentioned briefly as the best of the general sorts</li>
          <li>Unlike merge sort, it is not recursive</li>
          <li>Many implementations have been made that improve upon nlogn that are fairly complex so generally we use one of the best of these</li>
          <li>Based on picking a Pivot value and spliting based on the pivot</li>
        </ol>
      <li>Bucket Sort</li>
        <ol style="list-style-type: lower-roman;">
          <li>Uses indexing</li>
          <li>No longer doing compares with neighbors, key is applied to determine what "bucket" to place value in</li>
          <li>Not covered in-depth in lecture</li>
        </ol>
      <li>Counting Sort</li>
        <ol style="list-style-type: lower-roman;">
          <li>Uses indexing</li>
          <li>Index built as the contiguous values from n(min) to n(max)</li>
          <li>Works best when n is close to the size of the key (verify)</li>
          <li>Pass 1: Get min/max for index</li>
          <li>Pass 2: In a count array of equal size to index, store the counts of each value</li>
          <li>Pass 2b: Create a sum array of equal size to index, pass over count array and insert into sum array the running sum of count array values</li>
          <li>Pass 3: Insert into output array (size n) each value n into the array position equal to the sum at the position in the sum array of the value of n and then update the sum value in the sum array by +1 at that position (difficult to explain, easy to see)</li>
          <li>So, if the value of n is 25 partway through the list, then look at the sum value in the sum array at position 25 (let say it's 6).  Then put value 25 into the output array at position 6.  Then update the value in the sum array at position 25 to 7.  Now continue to the next n and repeat.</li>
          <li>One guy in the class said he came up with this on his own when Buckland asked us at the beginning of the course to invent our own sorts...hmmm...I doubt you would come up with this without creating at least a few other sorts first - I estimate about a minimum of 3 weeks on a student's schedule - he must be a genius (or more likely, he had outside knowledge)</li>
        </ol>
      <li>Create your own sort to try to improve on nlogn complexity</li>
        <ol style="list-style-type: lower-roman;">
          <li>Buckland is emphasising this as an important process to learning data structures.  Do it before you are "corrupted" by the knowledge of well-honed sorting techniques</li>
          <li>I definitely see the value in this.  I have to do it now.  I created my average sort at the beginning which was similar to merge sort and on the way towards quick sort but I need to try the next step and really attack specific case scenarios and what optimizations could I make in those cases BEFORE I see the amazing sorts already created by the world at large and never have to put my brain through this discovery process</li>
        </ol>
      <li>My next sort</li>
        <ol style="list-style-type: lower-roman;">
          <li>List of possible starting assumptions</li>
            <ol>
              <li>Required to reverse-sort on list that is known to be sorted</li>
              <li>Required to sort a list that has a small number of values out of order</li>
              <li>Required to reverse-sort on list that has a small number of values out of order</li>
              <li>List has no duplication of values (combine with other assumptions)</li>
              <li>List has a high number of duplicate values</li>
              <li>List has a low number of duplicate values (combine with other assumptions)</li>
              <li>Distribution is very likely to be around a small number of unknown points</li>
              <li>Distribution is going to be around a small number of known points</li>
              <li>Distribution is very likely to have void zones around unknown points</li>
              <li>Distribution is very likely to have void zones around known points</li>
              <li>Distribution is very likely to follow a known formula</li>
              <li>System attributes are known for sorting computer</li>
            </ol>
          <li>Easiest cases first</li>
            <ol>
              <li>Small unsorted list appended to known, sorted:  Size n is known, list is sorted except for a known number of entries at beginning or end of list</li>
              <li>Reverse-sort a sorted list</li>
              <li></li>
            </ol>
          <li>Sorts for easiest cases</li>
            <ol>
              <li>Small unsorted list appended to known, sorted:  <b>Insertion Sort</b> starting at 1st unsorted element</li>
              <li>Reverse-sort a sorted list:  <b>loop</b> - reverse traversal of list to insert in forward traversal of output list (simple loop, not a sort)</li>
              <li></li>
            </ol>
          <li>My second sort -> Counting sort with loop (instead of running sum)</li>
            <ol>
              <li>run through list n to get min and max</li>
              <li>create count array of size max - min + 1, values all zero</li>
              <li>run through list n and add +1 to value in countarray[n[i]-min] </li>
              <li>create output array of size n</li>
              <li>for each element in count array, add element position + min into successive slots in output array for each value of count array (inner loop)</li>
              <li>Prediction: more efficient than counting sort shown by Buckland because it involves one less pass (and one less array).</li>
            </ol>
        </ol>
      <li></li>
    </ol>
</ol>
<br />
<br />
<h2>The Art of Computer Programming</h2>
<span style="font-size:10px">(reference, title, chapter, point and footnote refer to <a href="/practices/feedbackloop">feedback loop</a> and NOT those elements in TAOCP)</span>
<br />
*Note: The feedback loop has been modified to match "chapter" with chapters in the book to avoid confusuion, "section" was added to be the level above "point"
<b>Feedback loop:  </b>

<ol>
  <li>Since this book is already organized for teaching, I will only do the following feedback procedures:
    <ol style="list-style-type:lower-alpha">
      <li>Notes</li>
      <li>Summary (end of chapter, after exercises, summary on what I learned)</li>
    </ol>
</ol>
<b>Reference: <i>The Art of Computer Programming</i>: Volume 1</i></b<br />
<ol>
  <li><b>Chapter: TOACP Vol 1 - Preface</b></li>
    <ol style="list-style-type: lower-alpha;">
      <li><b>Section: Pre-requisites: </b></li>
	      <ol style="list-style-type: lower-roman;">
	        <li>Point:  Very basic programming</li>
		      <li>Point:  Exposure to assembler recommended.</li>
        </ol>
	    <li><b>Section: Subject of this Volume</b></li>
	      <ol style="list-style-type: lower-roman;">
	        <li>Point:  Boring name: Non-numerical Analysis</li>
	        <li>Point:  Good name:  Analysis of Algorithms</li>
	        <li>Point:  Knuth's Description:  The theory of the properties of particular computer algorithms</li>
        </ol>
	    <li><b>Section: Goals</b></li>
	      <ol style="list-style-type: lower-roman;">
	        <li>Point:  Show that building a computer program from a set of instructions is like building a mathematical proof from a set of axioms.</li>
		      <li>Point:  Choose machine-oriented language over algebraic language (ex: ALGOL, FORTRAN) to most accurately describe a computer algorithm</li>
        </ol>
	    <li><b>Section: MIX</b></li>
	      <ol style="list-style-type: lower-roman;">
	        <li>Point:  USE MIX simulator to run programs done as excercises</li>
		      <li>Point:  </li>
        </ol>
	    <li><b>Section: Exercise Legend</b></li>
	      <ol style="list-style-type: lower-roman;">
	        <li>Score = 00:  Extremely easy, can almost always be done in your head.</li>
	        <li>Score = 10:  Simple problem to make you think about the material.  Probably take less than 1 minute.</li>
	        <li>Score = 20:  Average problem that may take 15 - 20 minutes.</li>
	        <li>Score = 30:  Moderate problem that may take a couple hours.</li>
	        <li>Score = 40:  Difficult or lengthy problem suitable for the classroom environment but reasonably solvable by the student.</li>
	        <li>Score = 50:  Research level problem with no current satisfactory solution available.</li>
		      <li>Point:  Note that this is a logarithmic scale of complexity.  A score of 48 may require an extremely advanced level to solve whereas a 45 might be readily (if not quickly) solvable by any computer scientist.
		      <li>Math Indicator = M:  Contains grade school level math</li>
		      <li>Math Indicator = HM: Contains higher level math but not necessarily difficult </li>
		      <li>Arrowhead >:  Recommended.  Very instructional.  At the minimum, all recommended, and scores <= 10 should be done.</li>
        </ol>
      </ol>
    <li><b>Chapter: TOACP Vol 1 - Basic Concepts (chapter 1) </b></li>
      <ol style="list-style-type: lower-alpha;">
        <li><b>Section: Algorithm notation: </b></li>
	        <ol style="list-style-type: lower-roman;">
	          <li>Example: E1 (means Euclid's Algorithm step 1)</li>
		        <li>Example: 1.1E1 (same as E1 but referenced outside chapter 1 so chapter and section included)</li>
		        <li>Example: m <- n (m gets assigned the value of n)</li>
		        <li>Algorithms start at step 1 and steps are sequential unless otherwise noted<li>
		        <li>Example:  Algorithm prefaced with condition like If r = 0, ... but condition fails (r != 0)**, then no action is taken (...)</li>
		        <li>Heavy vertical line (II)** indicates end of algorithm</li>
		        <li>For indexed items (v1, v2, v3, ... vn)** , a notation of v[j] could be used to represent the jth element (vj)**  and, likewise, a2,3 ** could be a[2,3]
          </ol>
        <li>First algorithm</li>
	        <ol style="list-style-type: lower-roman;">
	          <li>Always attempt on paper first!</li>
	          <li>Use algorithm E (Euclid's Algorithm of page 1), m = 119, n=544</li>
	          <li>Paraphrased description and solved on paper -> see <a href="#Excercises">Excercises</a> section
	        </ol>
        <li><b>5 Important Features of an Algorithm:</b>
	        <ol style="list-style-type: lower-roman;">
	          <li><b>Finiteness.</b>  An algorithm must always terminate after a finite number of steps.</li>
	          <li><b>Definiteness.</b>  Completely unambiguous - computers can't guess</li>
	          <li><b>Input.</b>  Zero or more <i>inputs</i> given prior to execution or during.</li>
	          <li><b>Output.</b>  One or more <i>outputs</i> relating to the inputs via the algorithm.</li>
	          <li><b>Effectiveness.</b>  Operations must all be sufficiently basic that they can in principle be done exactly and in a finite length of time by someone using pencil and paper. </li>
	        </ol>
	      <li><b>Elegance</b> in an algorithm:  <b>Algorithmic Analysis</b> -> for any algorithm, we want to evaluate it's performance characteristics.</li>
        <li><b>Formal definition of an algorithm</b></li>
           <ol style="list-style-type: lower-roman;">
	          <li><b>Computational methad</b>:  represented by the quadruple (Q,I,Omega,F)</li>
	          <li><b>Q</b> is a set containing subsets <b>I</b> and <b>Omega</b></li>
	          <li><b>F</b> is a function from <b>Q</b> into itself (endomorphic)</li>
	          <li><b>F</b></li> should leave <b>Omega</b> pointwise fixed:  F(q) = q  for all elements q of <b>Omega</b> <b>Note to self:  I don't get it.  I was good up to here.</b>
	        </ol>
      </ol>
  </ol><br />
<br />
<b>Footnotes:</b><br />
<ol>
  <li>** to indicate notation change because I can't reproduce the symbol(s) used in the book</li>
  <li>Alright, lost my paper notes and I don't feel like writing notes twice at the moment, so I'm going back to taking notes directly on the computer - for now</li>
  <li>TAKE PAPER NOTES FIRST from now on - transcribe before taking a break.  Noted while reading page 3</li>
  <li>Side-track: Einstein's Theory for the diffusion equation for Brownian particles...How I got there:  Knuth mentioned <b>Stochastic properties of particular algorithms</b> as a "real" world application of the computer-friendly algorithmic math that a "purist" mathematician would enjoy...So I had to look up <a href="http://en.wikipedia.org/wiki/Stochastic_properties">Stochastic properties</a> which lead to <a href="http://en.wikipedia.org/wiki/Brownian_motion">Brownian motion</a>, which I remembered from that famous pollen-jiggling-on-water-proof-of-atom-existance I learned years ago.  After an external interuption though, I figured I'd better get back to the book before losing my place in the book.</li>
  <li>While reading Knuth's defense for using machine-oriented language MIX to write the books computer algorithms, I thought to myself that it was this choice that really made me choose to read his book over a more modern language approach to his subject matter - losing touch with machine instructions is uncomfortable for a computer scientist imo, even though (or maybe especially because) I really prefer to work mostly with highly abstracted language.</li>
</ol>
<br />
<a name="Excercises">&nbsp;</a><br />
<b>Excercises</b>
<ol>
  <li>Chapter 1 - Basic Concepts</li>
    <ol style="list-style-type: lower-alpha;">
      <li>Solve E for m = 119, n = 544</li>
      <li>Algorithm description:</li>
	      <ol style="list-style-type: lower-roman;">
	        <li>E (Euclid's Algorithm)</li>
	        <li>given n,m where n>0, m>0</li>
	        <li>find greatest x where remainder r = 0 for n/x and m/x</li>
	      </ol>
	    <li>Algorithm Solution Steps:</li>
	      <ol style="list-style-type: lower-roman;">
	        <li>E1: m/n , get remainder r</li>
	        <li>E2: If r = 0, x = n, exit</li>
	        <li>E3: m<-n, n<-r, return to E1
	      </ol>
	    <li>Solution for m = 119, n = 544:</li>
	      <ol style="list-style-type: lower-roman;">
	        <li>E1: m/n = 119/544, r = 119</li>
	        <li>E2: If r = 0, false</li>
	        <li>E3: m<-544, n<-119</li>
	        <li>E1: m/n = 544/119, r = 68</li>
	        <li>E2: If r = 0, false</li>
	        <li>E3: m<-119, n<-68</li>
	        <li>E1: m/n = 119/68, r = 51</li>
	        <li>E2: If r = 0, false</li>
	        <li>E3: m<-68, n<-51</li>
	        <li>E1: m/n = 68/51, r = 17</li>
	        <li>E2: If r = 0, false</li>
	        <li>E3: m<-51, n<-17</li>
	        <li>E1: m/n = 51/17, r = 0</li>
	        <li>E2: If r = 0, true, x = 17</li>
	      </ol>
	  </ol>
</ol>  
	    
	    
	    
	      
      
  
</p>


